{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jk2pb1fRThvj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q langchain==0.0.152 pyllamacpp==1.0.7 sentencepiece gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain.llms import GPT4All\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "U7ekZu00Tyrt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
        "\n",
        "# send a GET request to the URL to download the file.\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# open the file in binary mode and write the contents of the response\n",
        "# to it in chunks.\n",
        "with open(local_path, 'wb') as f:\n",
        "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "        if chunk:\n",
        "            f.write(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iavySzJITylo",
        "outputId": "c1f15750-308b-424f-8f2a-1bdcde44f037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "514266it [07:17, 1176.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!cd llama.cpp && git checkout 2b26469\n",
        "!python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f37qUvFhTydS",
        "outputId": "10c04dfb-e243-4675-a597-22d204b6853b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 14571, done.\u001b[K\n",
            "remote: Counting objects: 100% (4617/4617), done.\u001b[K\n",
            "remote: Compressing objects: 100% (282/282), done.\u001b[K\n",
            "remote: Total 14571 (delta 4462), reused 4386 (delta 4332), pack-reused 9954\u001b[K\n",
            "Receiving objects: 100% (14571/14571), 16.47 MiB | 22.98 MiB/s, done.\n",
            "Resolving deltas: 100% (10187/10187), done.\n",
            "Note: switching to '2b26469'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 2b26469 convert.py: Support models which are stored in a single pytorch_model.bin (#1469)\n",
            "Loading model file models/gpt4all-lora-quantized-ggml.bin\n",
            "Writing vocab...\n",
            "[  1/291] Writing tensor tok_embeddings.weight                  | size  32001 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[  3/291] Writing tensor output.weight                          | size  32001 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "Wrote models/ggml-model-q4_0.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=['question'])"
      ],
      "metadata": {
        "id": "6KUTTFMqWdJC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llm = GPT4All(model=\"/content/models/ggml-model-q4_0.bin\",\n",
        "              callback_manager=callback_manager,\n",
        "              verbose=True)\n",
        "llm_chain = LLMChain(prompt = prompt, llm=llm)"
      ],
      "metadata": {
        "id": "al5DVxXuWdBr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What happens when it rains somewhere?\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "c1u2s7usXVtl",
        "outputId": "44cc036a-7cac-40dc-acf0-7d0a5f0884f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: What happens when it rains somewhere?\n",
            "\n",
            "Answer: Let's think step by step. When there is rain, firstly the earth gets water from clouds and finally reaching land through precipitation which also includes snowfall or sleet as well if that area has mountains like Himalayas. In places where trees are found, they get their nutrition through airborne pollens too (if it's spring season) in addition to other factors such as evaporation and transpiration of plants themselves can also play a key role for rainfall formation!"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Question: What happens when it rains somewhere?\\n\\nAnswer: Let's think step by step. When there is rain, firstly the earth gets water from clouds and finally reaching land through precipitation which also includes snowfall or sleet as well if that area has mountains like Himalayas. In places where trees are found, they get their nutrition through airborne pollens too (if it's spring season) in addition to other factors such as evaporation and transpiration of plants themselves can also play a key role for rainfall formation!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##writing poems\n",
        "question = \"Write a poem about friendship that rhymes.\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "BzSikN_Ojh8_",
        "outputId": "c1e753b2-89fa-47d9-c855-4198d8440bff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: Write a poem about friendship that rhymes.\n",
            "\n",
            "Answer: Let's think step by step.  First, we need to come up with some ideas for the theme of our Friendship Rhyme Poem which is \"friendship.\" Well! I could start out my friendships journey by mentioning how happy it makes me when they are around sharing their warmth and cheerfulness that radiates like a starlight in darkest night. Then, we can move on to describe the ways our friends bring joy into our lives--whether through laughter or music, books or movies. As I write this poem about friendship rhymes let's also include some examples of how they support and motivate us when life gets tough! And finally, don’t forget a thoughtful ending that reminds the reader to treasure their friends while they still have them because goodbye is never an easy decision."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Question: Write a poem about friendship that rhymes.\\n\\nAnswer: Let\\'s think step by step.  First, we need to come up with some ideas for the theme of our Friendship Rhyme Poem which is \"friendship.\" Well! I could start out my friendships journey by mentioning how happy it makes me when they are around sharing their warmth and cheerfulness that radiates like a starlight in darkest night. Then, we can move on to describe the ways our friends bring joy into our lives--whether through laughter or music, books or movies. As I write this poem about friendship rhymes let\\'s also include some examples of how they support and motivate us when life gets tough! And finally, don’t forget a thoughtful ending that reminds the reader to treasure their friends while they still have them because goodbye is never an easy decision.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}